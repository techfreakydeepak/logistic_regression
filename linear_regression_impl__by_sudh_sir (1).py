# -*- coding: utf-8 -*-
"""linear_regression_impl_ by sudh sir.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zvXFjIGWh5KjD4IBEWlNvwEsaRzs1_qv
"""

import pandas as pd
import matplotlib.pyplot as plt
import pickle
import numpy as np

!pip install -U ydata-profiling # HERE we use and install for the pip install and to find the pandas profiling report here

# prompt: drive mount

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv("/content/drive/MyDrive/linear regression live coding by sudh air /advertising.csv")

df

df.head()

df.tail()

import pandas_profiling # importig pandas_profiling and call the reports . ProfileReport(df)
pandas_profiling.ProfileReport(df)

# convcert all values and save to all to the varibales
pf=pandas_profiling.ProfileReport(df) # we use pf variable here to stored the values

pf

pf.to_widgets

pf.to_widgets() # making a report to do eda

pf.to_file("test.html")

# prompt: write code to show this report

from IPython.display import HTML

# Display the report as HTML
display(HTML(pf.to_html()))



# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set the correct file path
output_file = "/content/drive/MyDrive/linear regression live coding by sudh air/output.html"

# Create the necessary directories if they don't exist
!mkdir -p "/content/drive/MyDrive/linear regression live coding by sudh air"

# Save the report
pf.to_file(output_file)

df

x=df[["TV"]] #[[]] this is used to convert 1d to 2d
x

y = df.Sales

y

from sklearn.linear_model import LinearRegression

linear= LinearRegression()

linear.fit(x,y)

linear.intercept_ # C value of the equation y=mx+c

linear.coef_

file="linear_reg.sav"
pickle.dump(linear,open(file,'wb')) ## saving a model

linear.predict([[45]]) ## checking the sell of tv after spending 45 dollar on adv. on tv

##  some one give the this list of  expen. for the tv adv.
l=[4,5,6,85,39,45]

for i in l:
  print(linear.predict([[i]]))

saved_model=pickle.load(open(file,'rb')) # opening the saved model that we make few time ago

saved_model.predict([[45]])

linear.score(x,y) #  checking the model accuary

df

data_x=df[["TV",	"Radio",	"Newspaper"]]

y = df.Sales
y

lm =LinearRegression()
lm.fit(data_x,y)

lm.intercept_

lm.coef_

lm.score(data_x,y)

data_x=df[["TV",	"Radio"]]
y=df.Sales

lm1=LinearRegression()
lm1.fit(data_x,y)
lm1.score(data_x,y)

import statsmodels.formula.api as smf
lm=smf.ols(formula='Sales~TV',data=df).fit()#ordienary least squared error ols
lm.summary()

import statsmodels.formula.api as smf
lm=smf.ols(formula='Sales~TV+Radio',data=df).fit()
lm.summary()

lm=smf.ols(formula='Sales~TV+Radio+Newspaper',data=df).fit() # here we check the p  value here in which we feature selection on it
lm.summary()

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge ,Lasso ,RidgeCV,LassoCV,ElasticNet,ElasticNetCV,LinearRegression
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# prompt: drive mount

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv("/content/drive/MyDrive/Admission_Prediction.csv")

df

!pip install -U ydata-profiling # HERE we use and install for the pip install and to find the pandas profiling report here

import pandas_profiling # importig pandas_profiling and call the reports . ProfileReport(df)
pandas_profiling.ProfileReport(df)

# convcert all values and save to all to the varibales
pf=pandas_profiling.ProfileReport(df) # we use pf variable here to stored the values

pf.to_widgets

pf.to_widgets()

df["GRE Score"]=df["GRE Score"].fillna(df["GRE Score"].mean()) # fill the missing value  with the average of the particular group

df["TOEFL Score"]=df["TOEFL Score"].fillna(df["TOEFL Score"].mean())

df["University Rating"]=df["University Rating"].fillna(df["University Rating"].mean())

df.describe()

df.isnull().sum()

df.drop(columns=["Serial No."],inplace=True)

df

x=df[["GRE Score","TOEFL Score","University Rating",	"SOP",	"LOR",	"CGPA",	"Research"]]

# x= df.drop(columns=["Chance of admit "]) we also did this when there is large number of columns are present

x

y=df["Chance of Admit"]

y

scaler=StandardScaler() # why we need standard scaler

scaler.fit_transform(x) ## fit my feature selcetion or x value its also called as z test

arr=scaler.fit_transform(x)

arr

df1=pd.DataFrame(arr)

df1.profile_report()

df1

df1.describe()

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_df =pd.DataFrame()# vif_df fucntion is created

vif_df["vif"]=[variance_inflation_factor(arr,i) for i in range (arr.shape[1])]

vif_df["feature"]=x.columns

vif_df

arr

# dividing the dataset after the standard scaler  and randomness is fixed here
x_train,x_test,y_train,y_test=train_test_split(arr,y,test_size=0.25,random_state=345) # arr is x after the standardscaler split the dataset for the trainig and test

x_train

linear=LinearRegression()

linear.fit(x_train,y_train)

import pickle
pickle.dump(linear,open("admission_linear_model.pickle","wb"))

!ls

df

linear.predict([[337.000000,	118.0	,4.0,	4.5,	4.5,	9.65,	1]])

test1=scaler.transform([[322.000000	,110.0	,3.0	,3.5	,2.5	,8.67	,1	]])

test1

## now do the prediction after the sclar standartion  we have to do the same things with my testing and train dataset
linear.predict([[ 0.49051785,  0.46519653, -0.1078766 ,  0.12727117, -1.06433187,
         0.15484742,  0.88640526]])

linear.predict(test1) #same wahi code hai jo upper likha hai

model=pickle.load(open("admission_linear_model.pickle",'rb'))# checking the model with  testing

model.predict(test1)

linear.score(x_test,y_test) # now giving the unknown dataset to check the score and giving the  test dataset

x_test

y_test

## lets create a fucntion to create adjusted R- squared
def adj_r2(x,y):
  r2=linear.score(x,y)
  n=x.shape[0]
  p=x.shape[1]
  adjusted_r2=1-(1-r2)*(n-1)/(n-p-1)
  return adjusted_r2

adj_r2(x_test,y_test) # the value of adj r2 is kept incresing when we inceasing the test data set and lower the randomness

linear.coef_

linear.intercept_

"""# lasso regularztion"""

#lassocv= LassoCV(cv=10,max_iter=2000000,normalize=True)
 #lassocv.fit(x_train,y_train)

lassocv = LassoCV(cv=10, max_iter=2000000)
lassocv.fit(x_train, y_train)

lassocv.alpha_

lasso=Lasso(alpha=lassocv.alpha_)
lasso.fit(x_train,y_train)

lasso.score(x_test,y_test)

lassocv = LassoCV(alphas=None,cv=5, max_iter=2000000)
lassocv.fit(x_train, y_train)





"""# now ridge l2 regulaztion"""

ridgecv=RidgeCV(alphas=np.random.uniform(0,10,50),cv=10)
ridgecv.fit(x_train,y_train)

ridgecv.alpha_

np.random.uniform(0,10,50) # we put this value on the place of the aplha_

ridge_lr=Ridge(alpha=ridgecv.alpha_)
ridge_lr.fit(x_train,y_train)

ridge_lr.score(x_test,y_test) ## accuaary with l1 ridge

"""# elastic"""

elastic=ElasticNetCV(alphas=None,cv=10)
elastic.fit(x_train,y_train)

elastic.alpha_

elastic.l1_ratio_

elastic_lr =ElasticNet(alpha=elastic.alpha_, l1_ratio=elastic.l1_ratio_)

elastic_lr.fit(x_train,y_train)

elastic_lr.score(x_test,y_test)

